{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import glob\n",
    "import random\n",
    "# 读取文件内容函数\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "        content = \"\"\n",
    "        for k in f:\n",
    "            content += k\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词频前10个\n",
    "def get_frequent(word_list, top_K=10):\n",
    "    tf_dic = {}\n",
    "    for i in range(len(word_list)):\n",
    "        tf_dic[word_list[i]] = tf_dic.get(word_list[i], 0) + 1\n",
    "    return sorted(tf_dic.items(), key=lambda x:x[1]*-1)[:top_K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取停顿词\n",
    "def get_stopword(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "        return [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    files = glob.glob(\"../learning-nlp/chapter-3/data/news/C000013/*3.txt\")\n",
    "    # 循环从txt文件中读取内容\n",
    "    content = [read_file(f) for f in files]\n",
    "    # 对内容进行结巴分词\n",
    "    content_cut = []\n",
    "    for i in content:\n",
    "        # print(i)\n",
    "        # print(type(jieba.cut(i))) 生成器类型\n",
    "        # a获取不是空格的分词\n",
    "        a = list(jieba.cut(i))\n",
    "        a = [word.strip() for word in a if word.strip() != \"\"]\n",
    "        # a不是停顿词\n",
    "        a = [x for x in a if x not in get_stopword(\"../learning-nlp/chapter-3/data/stop_words.utf8\")]\n",
    "        content_cut.extend(a)\n",
    "    rand_num = random.randint(0, len(content))\n",
    "    print(len(content))\n",
    "    print(\"样本:\", content[rand_num])\n",
    "    print(\"分词:\", \"/\".join(content_cut))\n",
    "    print(get_frequent(content_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
